{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will find documentation for code along the way. I have put them wherever necessary. For proper guidance and documentation, please follow this GitHub link.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an automatic image captioning model to predict reliable and appropriate captions for new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do We Need This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many researches in this field are already being done. Some probale benefits:\n",
    "1. Help people find relevant images faster on the internet, along with sources and direct website.\n",
    "2. Most importantly, it can help visually challenged people to know their locations easily. They can take pictures on the phone, the captions will be generated, and another machine learning model can read out those captions.(Possible future work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Flickr8K](https://forms.illinois.edu/sec/1713398) dataset is being used for this project. \n",
    "**Why Flickr8K:**\n",
    "* Large enough to get started to get considerable results and approximations about the trained model.\n",
    "* Not very large like the Flickr30k or [MSCOCO](http://cocodataset.org/#home) which require really huge amount of RAM and GPU power for getting good and reproducable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras\n",
    "* Matplotlib\n",
    "* VGG16\n",
    "* NLTK\n",
    "* TensorFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The pre-trained [VGG16](https://arxiv.org/abs/1409.1556) model is used to extract the features from the images.\n",
    "* Then the features are fed into an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) network for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A GPU is highly recommended if you intend to run this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in the dataset is annotated with one or more different captions. Although an image's annotation is image file's name, the different captions are distinguished by indices.\n",
    "\n",
    "Let's load the image captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the captions\n",
    "def load_captions_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Data/Flickr8k.token.txt'\n",
    "captions = load_captions_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`captions` contains all the tokens that are stores in the token file of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, each photo is identified by a unique name and the captions are indexed. So, we can get the description for each photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of photo descriptions:  8092\n",
      "key: , value:  ('1000268201_693b08cb0e', ['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .'])\n"
     ]
    }
   ],
   "source": [
    "# extracting the photo descriptions\n",
    "def get_descriptions(captions):\n",
    "    mapping = dict()\n",
    "    for line in captions.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        img_id, img_desc = tokens[0], tokens[1:]\n",
    "        # removing the .jpg extension\n",
    "        img_id = img_id.split('.')[0]\n",
    "        img_desc = ' '.join(img_desc)\n",
    "        # creating list and storing the descriptions\n",
    "        if img_id not in mapping:\n",
    "            mapping[img_id] = list()\n",
    "        mapping[img_id].append(img_desc)\n",
    "        \n",
    "    return mapping\n",
    "    \n",
    "\n",
    "descriptions = get_descriptions(captions)\n",
    "print('No. of photo descriptions: ', len(descriptions))\n",
    "# printing the description for one photo\n",
    "for key in descriptions:\n",
    "    print (\"key: , value: \", (key, descriptions[key]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean and tokenize the text data.\n",
    "Some of the steps:\n",
    "* Removing all words that have numbers in them.\n",
    "* Removing punctuations.\n",
    "* Removing single characters.\n",
    "* Converting each character to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: , value:  ('1000268201_693b08cb0e', ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin'])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(descriptions):\n",
    "    for key, all_desc in descriptions.items():\n",
    "        for i in range(len(all_desc)):\n",
    "            desc = all_desc[i]\n",
    "            # separating by white space\n",
    "            desc = desc.split()\n",
    "            # converting to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # removing punctuation\n",
    "            desc = [text_original.translate(str.maketrans('','',string.punctuation))\n",
    "                   for text_original in desc]\n",
    "            # removing single characters\n",
    "            desc = [word for word in desc if len(word) > 1]\n",
    "            # removing words with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # joining\n",
    "            all_desc[i] = ' '.join(desc)\n",
    "            \n",
    "clean_text(descriptions)\n",
    "\n",
    "# chekcing whether cleaned or not\n",
    "for key in descriptions:\n",
    "    print (\"key: , value: \", (key, descriptions[key]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the text cleaning process went fine.\n",
    "\n",
    "Next we can vocabularize the descriptions that we have. We can use a set to make it as small as possible and remove the duplicate elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  8763\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "vocabulary = get_vocabulary(descriptions)\n",
    "print('Vocabulary size: ', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 8763 unnique words for the vocabulary set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the descriptions of the images in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(descriptions, filename):\n",
    "    lines = list()\n",
    "    \n",
    "    for key, all_desc in descriptions.items():\n",
    "        for desc in all_desc:\n",
    "            lines.append(key + ' ' + desc)\n",
    "            only_desc.append(desc)\n",
    "        \n",
    "        text = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(text)\n",
    "        file.close()\n",
    "\n",
    "only_desc = [] # a list containing only the descriptions\n",
    "save_to_file(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data has been handled. Before moving on to the image preparation part, we can do some visualizations on the text data.\n",
    "\n",
    "**Let's see some of the most frequently occuring words in the dataset. We will be using wordcloud for this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# converting `only_desc` list to string\n",
    "only_desc_str = ''\n",
    "only_desc_str = only_desc_str.join(only_desc)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "wordcloud = WordCloud(width=800, height=800, margin=0, background_color=\"black\").generate(only_desc_str)\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a really good perspective about the most frequent words in our vocabulary. Next, we can move on to prepare the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Photo Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be extracting the features from the images using a pre-trained VGG16 model.  \n",
    "As we will be using Keras, we can directly load this model.  \n",
    "Also, we do not need all the layers of the VGG model. This is because the top layer of the model does classification and we are not interested in that. We will be removing the top layer and then connecting our own network. In this way, we can extract the image features ny using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Installed_Softwares\\Anaconda\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted  8091 features\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "\n",
    "# extracting the features from each photo\n",
    "def get_features(directory):\n",
    "    model = VGG16()\n",
    "    # removing the top layer\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    print(model.summary())\n",
    "    # extracting the features\n",
    "    features = dict()\n",
    "    for name in os.listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # coverting the image's pixels to array\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        image = preprocess_input(image)\n",
    "        # getting the features\n",
    "        feature = model.predict(image)\n",
    "        # getting image id\n",
    "        img_id = name.split('.')[0]\n",
    "        features[img_id] = feature\n",
    "    return features\n",
    "\n",
    "# executing the function\n",
    "directory = 'Data/Flicker8k_Dataset'\n",
    "features = get_features(directory)\n",
    "print('Extracted ', len(features), 'features')\n",
    "# saving the features into a .pkl file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code, we are using the VGG16  model (except the top) layer (line 11) to extract the features of the photographs (line 24). Then the features are stored in a `.pkl` file (line 36)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been divided into three different set, a training set (6000 images), a development set (1000 images) and a test set (1000) images.\n",
    "\n",
    "The file names are `Flickr_8k.trainImages.txt`, `Flickr_8k.devImages.txt` and `Flickr_8k.testImages.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all the Aailable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will be loading all the requried files viz. `Flickr_8k.trainImages.txt`, the clean data file `descriptions.txt` and the file for the extracted features of the images, `features.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading documents into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # image id\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be loading the the file containing the clean description for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_desc(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        img_id, img_desc = tokens[0], tokens[1:]\n",
    "        # skipping images that are not in the set\n",
    "        if img_id in dataset:\n",
    "            if img_id not in descriptions:\n",
    "                descriptions[img_id] = list()\n",
    "            desc = 'start' + ' '.join(img_desc) + ' end'            \n",
    "            descriptions[img_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code, we used the strings `start` and `end` to signal the start and end of the caption. This is beacuse the model generates the captions one word at a time.\n",
    "\n",
    "Next, we will be loading the `features.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the image features\n",
    "def load_image_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map the words to unique integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dictionary of descriptions to a list\n",
    "def to_list(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# creating unique integers for the tokens\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be learning by splitting the descriptions into text. It will be provided with a word and the photo and it will generate the next word. Then it will be fed with the two words along with the photo to generate the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our model will get the photo features and encoded text as inputs. The output will be the next encoded word. Also, the output will be one-hot encoded word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sequences for input images, encoded text and ouput words\n",
    "def create_sequences(tokenizer, max_length, desc_list, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        # encoding\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # splitting the sequence\n",
    "        for i in range(1, len(seq)):\n",
    "            # to input and output pairs\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # padding the input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # one-hot encoding the output\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                \n",
    "            X1.append(photos)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "                \n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the maximum length description\n",
    "def max_length(descriptions):\n",
    "    lines = to_list(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "We extract the photo features using the 16 layer VGG model. To handle the text input Long Short-Term Memory (LSTM) recurrent neural network will be used.\n",
    "Then, both the outputs (vectors) are merged together by a Dense layer (Decoder Model) to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu',)(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # merging and compiling\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # summarizing\n",
    "    print(model.summary())\n",
    "    # creates a plot and saves it to file\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is for memory managed fitting (less than 32GB of RAM)\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # getting the image features\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, \n",
    "                                                       max_length, \n",
    "                                                       desc_list, \n",
    "                                                       photo)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset:  6000\n",
      "Descriptions for train:  6000\n",
      "Images in trainset:  6000\n",
      "Tokenized Vocab size:  8305\n",
      "Description Length: 33\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# loading the training set\n",
    "filename = 'Data/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Trainset: ', len(train))\n",
    "# loading the clean descriptions\n",
    "train_descriptions = load_clean_desc('descriptions.txt', train)\n",
    "print('Descriptions for train: ', len(train_descriptions))\n",
    "# loading the photo features\n",
    "train_features = load_image_features('features.pkl', train)\n",
    "print('Images in trainset: ', len(train_features))\n",
    "\n",
    "\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Tokenized Vocab size: ', vocab_size)\n",
    "\n",
    "# calling the `max_length()` function\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 33, 256)      2126080     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 4096)         0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 33, 256)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          1048832     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          525312      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256)          0           dense_9[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          65792       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 8305)         2134385     dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,900,401\n",
      "Trainable params: 5,900,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 478s 80ms/step - loss: 4.7945\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 512s 85ms/step - loss: 3.9614\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 497s 83ms/step - loss: 3.6776\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 464s 77ms/step - loss: 3.5091\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 459s 77ms/step - loss: 3.3986\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 471s 78ms/step - loss: 3.3214\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 477s 80ms/step - loss: 3.2653\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 480s 80ms/step - loss: 3.2197\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 473s 79ms/step - loss: 3.1858\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 486s 81ms/step - loss: 3.1467\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 499s 83ms/step - loss: 3.1222\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 468s 78ms/step - loss: 3.1021\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 462s 77ms/step - loss: 3.0824\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 461s 77ms/step - loss: 3.0670\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 460s 77ms/step - loss: 3.0522\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 464s 77ms/step - loss: 3.0447\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 496s 83ms/step - loss: 3.0343\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 501s 83ms/step - loss: 3.0231\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 482s 80ms/step - loss: 3.0196\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 459s 76ms/step - loss: 3.0124\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# training and saving models after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, \n",
    "                               train_features, \n",
    "                               tokenizer, \n",
    "                               max_length)\n",
    "    # fitting for one epoch\n",
    "    model.fit_generator(generator, epochs=1, \n",
    "                       steps_per_epoch=steps, verbose=1)\n",
    "    # saving the model\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `model_19.h5` (the last one) has the lowest loss of 3.0124."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the fitting is complete, we can move on to evelauation now.  \n",
    "We will use the trained model to generate descriptions for the test dataset.\n",
    "\n",
    "For evaluation we will be using the BLEU score (close to 1.0 is better) that summarizes how close the generated text is to the expected text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word (to be called by `generate_desc`)\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # start the process\n",
    "    in_text = 'start'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predicting the next word\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # covert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # if unable to map word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # storing the actual and predicted text\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    \n",
    "    # calculate the BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions for test: 1000\n",
      "Images in test: 1000\n",
      "BLEU-1: 0.431691\n",
      "BLEU-1: 0.204005\n",
      "BLEU-1: 0.142773\n",
      "BLEU-1: 0.064348\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# loading test set\n",
    "filename = 'Data/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# loading descriptions\n",
    "test_descriptions = load_clean_desc('descriptions.txt', test)\n",
    "print('Descriptions for test: %d' % len(test_descriptions))\n",
    "# loading photo features\n",
    "test_features = load_image_features('features.pkl', test)\n",
    "print('Images in test: %d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = 'model_19.h5'\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features,\n",
    "              tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code, we get the following BLEU scores:\n",
    "BLEU-1: 0.431691  \n",
    "BLEU-1: 0.204005  \n",
    "BLEU-1: 0.142773  \n",
    "BLEU-1: 0.064348  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the tokenizer for encoding generated words for the model. Also, the maximum length if input sequences is used when defining the model.\n",
    "\n",
    "We will be creating a Tokenizer and save it as `tokenizer.pkl`. That will help us to load the tokenizer whenever we will need it. We can also hard code the maximum description length that we got above (33)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_desc(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "# load set\n",
    "filename = 'Data/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# load descriptions\n",
    "train_descriptions = load_clean_desc('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will generate caption for an entirely new photograph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start flying over the water end\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'start'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 33\n",
    "# load the model\n",
    "model = load_model('model_19.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features('example.jpg')\n",
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
